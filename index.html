<!DOCTYPE html>
<html lang="zh-cmn-Hans">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Xiaoqing Xie</title>
  
  
  
  <!--link rel="stylesheet" href="//cdn.jsdelivr.net/highlight.js/9.10.0/styles/github-gist.min.css"-->
  
<link rel="stylesheet" href="//cdn.jsdelivr.net/highlight.js/9.10.0/styles/github-gist.min.css">

  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.4.2"></head>

<body>
<div class="Shell">
    <aside class='SideBar'>
    <section class='avatar' style="background-image: url()">
        <div class='av-pic' style="background-image: url(/assets/myphoto.png)">
        </div>
    </section>
    <section class='menu'>
        <div>Xiaoqing Xie</div>
        
        <ul>
          
            <a href="/Resume/" class="Btn">
              <li>Resume</li>
            </a>  
          
            <a href="/" class="Btn">
              <li>Blogs</li>
            </a>  
          
            <a href="/archives/" class="Btn">
              <li>Archive</li>
            </a>  
          
            <a href="/tags/" class="Btn">
              <li>Tags</li>
            </a>  
          
            <a href="/categories/" class="Btn">
              <li>Categories</li>
            </a>  
          
            <a href="/about/" class="Btn">
              <li>About</li>
            </a>  
          
        </ul>
    </section>
    <section class="media">
        
            
                <a target="_blank" rel="noopener" href="https://github.com/XiaoqingXie">
                    <img src="/assets/github.svg" />
                </a>
            
        
    </section>
</aside>

    <div class="container">
        <div data-pager-shell>
            <ul class="Index">
  
  
    <li>
      <article class='ListView'>
    <header class="title">
      
        <h1>
          <a href="/2022/08/08/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%85%AC%E5%BC%80%E8%AF%BE/">学习笔记-清华大学大模型公开课</a>
        </h1>
      
      <div class='ListMeta'>
  <time datetime="2022-08-08T05:45:00.000Z" itemprop="datePublished">
    2022-08-08
  </time>
  
  
</div>

    </header>
    <div>
      
        <p>Course Plan</p>
<ol>
<li>大模型基础知识<br>Lesson1 NLP &amp; Big Model Basics（GPU server, Linux, Bash, Conda, …)<br>Lesson2 Neural Network Basics(Pytorch<br>Lesson3 Transformer and PLMs(Huggingface Transformers)</li>
<li>大模型的关键技术<br>Lesson4 Prompt Tuning &amp; Delta Tuning(OpenPrompt, OpenDelta)<br>Lesson5 Efficient Training &amp; Model Compression(OpenBMB suite)<br>Lesson6 Big-Model-based Text understanding and generation</li>
<li>大模型在交叉学科的应用<br>Lesson7 Big Models X Biomedical Science<br>Lesson8 Big Models X Legal Intelligence<br>Lesson9 Big Models X Brain and Cognitive Science</li>
</ol>
<p>Lesson1<br>Basic Tasks of NLP<br>Part of speech, Named entity recognition, Co-reference, Basic dependencies</p>

      
    </div>
</article>

    </li>
  
    <li>
      <article class='ListView'>
    <header class="title">
      
        <h1>
          <a href="/2022/07/21/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Java/">学习笔记-Java</a>
        </h1>
      
      <div class='ListMeta'>
  <time datetime="2022-07-21T11:31:54.000Z" itemprop="datePublished">
    2022-07-21
  </time>
  
  
</div>

    </header>
    <div>
      
        
      
    </div>
</article>

    </li>
  
    <li>
      <article class='ListView'>
    <header class="title">
      
        <h1>
          <a href="/2022/07/19/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Tensorflow/">学习笔记-Tensorflow</a>
        </h1>
      
      <div class='ListMeta'>
  <time datetime="2022-07-19T08:25:22.000Z" itemprop="datePublished">
    2022-07-19
  </time>
  
  
</div>

    </header>
    <div>
      
        
      
    </div>
</article>

    </li>
  
    <li>
      <article class='ListView'>
    <header class="title">
      
        <h1>
          <a href="/2022/07/19/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Pytorch/">学习笔记-Pytorch</a>
        </h1>
      
      <div class='ListMeta'>
  <time datetime="2022-07-19T08:25:10.000Z" itemprop="datePublished">
    2022-07-19
  </time>
  
  
</div>

    </header>
    <div>
      
        <p><strong>Pytorch vs Tensorflow</strong><br>Pytorch:简洁、动态计算、visdom、部署不方便<br>Tensorflow: 接口复杂、静态图(TF2.0 Eager Execution已经引入动态图)、Tensorboard、部署方便(TF serving)</p>
<p><strong>Pytorch安装与环境搭建</strong><br>Ubuntu16.04–CUDA+cuDNN–Python3+pip3/Anaconda–Pytorch<br>硬件要求：GTX1080Ti+16G内存</p>
<p><strong>Pytorch的基本概念</strong></p>
<ol>
<li><p>Tensor 张量<br>&emsp;使用Tensor对样本进行描述<br>&emsp;使用Tensor对模型中的变量进行描述<br>&emsp;Tensor的创建<br>&emsp;Tensor(*size)    基础构造函数<br>&emsp;Tensor(data)    类似np.array<br>&emsp;标量是零维的张量，向量是一维的张量，矩阵是二维的张量。<br>&emsp;<strong>Tensor可以用来描述机器学习中的样本或者模型。</strong></p>
</li>
<li><p>Tensor创建编程实例</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.Tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a.<span class="built_in">type</span>())</span><br></pre></td></tr></table></figure>
<br>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a = torch.Tensor(<span class="number">2</span>,<span class="number">3</span>) <span class="comment"># 指定tensor的形状</span></span><br><span class="line">b = torch.ones(<span class="number">2</span>,<span class="number">2</span>) <span class="comment"># 指定形状，初始化为全1的tensor</span></span><br><span class="line">c = torch.eye(<span class="number">2</span>,<span class="number">2</span>) <span class="comment"># 指定形状，初始化为对角线的tensor</span></span><br><span class="line">d = torch.zeros(<span class="number">2</span>,<span class="number">2</span>) <span class="comment"># 指定形状，初始化为全0的tensor</span></span><br><span class="line">a1 = torch.zeros_like(a) <span class="comment"># 以tensor a的形状，初始化一个全0的tensor</span></span><br><span class="line">a2 = torch.ones_like(a)</span><br><span class="line">a3 = torch.eye_like(a)</span><br><span class="line"><span class="comment"># 随机</span></span><br><span class="line">e = torch.rand(<span class="number">2</span>,<span class="number">2</span>) <span class="comment"># 指定tensor的形状,生成随机值的tensor</span></span><br><span class="line">f = torch.normal(mean=<span class="number">0.0</span>, std=torch.rand(<span class="number">5</span>))</span><br><span class="line">g = torch.Tensor(<span class="number">2</span>,<span class="number">2</span>).uniform(-<span class="number">1</span>,<span class="number">1</span>) <span class="comment">#先指定形状，再定义从-1到1之间的均匀分布tensor</span></span><br><span class="line"><span class="comment"># 序列</span></span><br><span class="line">h = torch.arange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">1</span>) <span class="comment"># 从0-9的间隔1的tensor(long tensor)</span></span><br><span class="line">i = torch.linspace(<span class="number">2</span>, <span class="number">10</span>, <span class="number">3</span>) <span class="comment"># 等差tensor</span></span><br><span class="line">j = torch.randperm(<span class="number">10</span>) <span class="comment"># 从0-9打乱的tensor(long tensor)</span></span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;numpy定义数据的实例<br>&emsp;&emsp;用法与Tensor类似</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure>
<br>

<ol start="3">
<li>Tensor的属性</li>
</ol>
<p>&emsp;&emsp;每一个Tensor有torch.dtype、torch.device、torch.layout三种属性。<br>&emsp;&emsp;torch.device标识了torch.Tensor对象再创建后所存储在的设备名称(cpu/gpu)。<br>&emsp;&emsp;torch.layout表示torch.Tensor内存布局对象。<br>&emsp;&emsp;例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tnesor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],dtype=torch.float32,device=torch.device(<span class="string">&#x27;cpu&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;稀疏的张量的定义<br>&emsp;&emsp;torch.sparse_coo_tensor<br>&emsp;&emsp;coo类型表示了非零元素的坐标形式<br>&emsp;&emsp;例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">dev = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">indices = torch.tensor([<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]))  <span class="comment"># 坐标</span></span><br><span class="line">values = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])。<span class="comment"># 值</span></span><br><span class="line">x = torch.sparse_coo_tensor(indices, values, [<span class="number">4</span>,<span class="number">4</span>], dtype=torch.float32, device=dev) <span class="comment"># 对角线上有值的稀疏张量</span></span><br><span class="line">x = torch.sparse_coo_tensor(indices, values, [<span class="number">4</span>,<span class="number">4</span>], dtype=torch.float32, device=dev)。to_dense()  <span class="comment"># 转换成稠密张量</span></span><br></pre></td></tr></table></figure>
<br>

<ol start="4">
<li>Tensor的算术运算</li>
</ol>
<p>&emsp;&emsp;<strong>四则运算</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加法</span></span><br><span class="line">c = a + b</span><br><span class="line">c = torch.add(a, b)</span><br><span class="line">a.add(b)</span><br><span class="line">a.add_(b) <span class="comment"># 会对a的值修改</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 减法</span></span><br><span class="line">c = a - b</span><br><span class="line">c = torch.sub(a, b)</span><br><span class="line">a.sub(b)</span><br><span class="line">a.sub_(b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 乘法(哈达玛积, element wise, 对应元素相乘)</span></span><br><span class="line">c = a * b</span><br><span class="line">c = torch.mul(a, b)</span><br><span class="line">a.mul(b)</span><br><span class="line">a.mul_(b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 除法</span></span><br><span class="line">c = a / b</span><br><span class="line">c = torch.div(a, b)</span><br><span class="line">a.div(b)</span><br><span class="line">a.div_(b)</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;<strong>矩阵运算</strong><br>&emsp;&emsp;二维矩阵乘法运算操作包括torch.mm()、torch.matmul()、@</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">b = torch.ones(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">torch.mm(a, b)</span><br><span class="line">torch.matmul(a, b)</span><br><span class="line">a @ b</span><br><span class="line">a.matmul(b)</span><br><span class="line">a.mm(b)</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;对于高位的Tensor(dim&gt;2)，定义其矩阵乘法仅在最后的两个维度上，要求前面的维度必须保持一致，就像矩阵的索引一样并且运算操作只有torch.matmul().</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">b = torch.ones(<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a.matmul(b))</span><br><span class="line"><span class="built_in">print</span>(torch.matmul(a,b))</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;幂运算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">pow</span>(a, <span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(a.<span class="built_in">pow</span>(<span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(a**<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(a.pow_(<span class="number">2</span>)) <span class="comment"># 结果赋给a</span></span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;e的n次方</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.exp(a))</span><br><span class="line">b = a.exp_()</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;开方运算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(a.sqrt())</span><br><span class="line"><span class="built_in">print</span>(a.sqrt_())</span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;对数运算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.log2(a))</span><br><span class="line"><span class="built_in">print</span>(torch.log10(a))</span><br><span class="line"><span class="built_in">print</span>(torch.log(a))</span><br><span class="line"><span class="built_in">print</span>(torch.log_(a))</span><br></pre></td></tr></table></figure>
<br>

<ol start="5">
<li>Pytorch的广播机制</li>
</ol>
<p>&emsp;&emsp;<strong>广播机制：</strong>张量参数可以自动扩展为相同大小<br>&emsp;&emsp;广播机制需要满足两个条件：<br>&emsp;&emsp;&emsp;1)每个张量至少有一个维度；<br>&emsp;&emsp;&emsp;2)满足右对齐:对两个张量的维度从后往前（从右向左）处理，维度的大小必须<strong>要么相等，要么其中一个为1</strong>，或者其中一个张量后面不存在维度了；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.empty(<span class="number">5</span>,<span class="number">7</span>,<span class="number">3</span>)</span><br><span class="line">y = torch.empty(<span class="number">5</span>,<span class="number">7</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 形状相同，可以广播</span></span><br><span class="line"></span><br><span class="line">x=torch.empty((<span class="number">0</span>,))</span><br><span class="line">y=torch.empty(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 不能广播，因为两个张量都必须只有一个维度</span></span><br><span class="line"></span><br><span class="line">x=torch.empty(<span class="number">5</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">y=torch.empty(  <span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 可以广播</span></span><br><span class="line"><span class="comment"># 1st trailing dimension: both have size 1</span></span><br><span class="line"><span class="comment"># 2nd trailing dimension: y has size 1</span></span><br><span class="line"><span class="comment"># 3rd trailing dimension: x size == y size</span></span><br><span class="line"><span class="comment"># 4th trailing dimension: y dimension doesn&#x27;t exist</span></span><br><span class="line"></span><br><span class="line">x=torch.empty(<span class="number">3</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">y=torch.empty(  <span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># x和y不能广播，因为倒数第三个维度大小不同，且不为1</span></span><br></pre></td></tr></table></figure>
<br>

<ol start="6">
<li>Tensor的取整/取余运算</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"> </span><br><span class="line">a = torch.rand(<span class="number">2</span>, <span class="number">2</span>).mul_(<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 向下取整</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;floor :&#x27;</span>,torch.floor(a))</span><br><span class="line"><span class="comment"># 向上取整</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;ceil :&#x27;</span>,torch.ceil(a))</span><br><span class="line"><span class="comment"># 四舍五入</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;round :&#x27;</span>,torch.<span class="built_in">round</span>(a))</span><br><span class="line"><span class="comment"># 只取整数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;trunc :&#x27;</span>,torch.trunc(a))</span><br><span class="line"><span class="comment"># 只取小数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;frac :&#x27;</span>,torch.frac(a))</span><br><span class="line"><span class="comment"># 取余数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;% :&#x27;</span>,a%<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<br>

<ol start="7">
<li>Tensor的比较运算</li>
</ol>
<p>&emsp;&emsp;等于、大于等于、大于、小于等于、小于、不相等</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x = torch.Tensor([[<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>],[<span class="number">4</span>,<span class="number">7</span>,<span class="number">9</span>]])</span><br><span class="line">y = torch.Tensor([[<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">4</span>,<span class="number">8</span>,<span class="number">9</span>]])</span><br><span class="line">z = torch.Tensor([[<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>],[<span class="number">4</span>,<span class="number">7</span>,<span class="number">9</span>]])</span><br><span class="line"><span class="built_in">print</span>(torch.eq(x,y))</span><br><span class="line"><span class="comment"># 比较两个Tensor是否相等</span></span><br><span class="line"><span class="built_in">print</span>(torch.equal(x,z))</span><br><span class="line"><span class="built_in">print</span>(torch.equal(x,y))</span><br><span class="line"><span class="comment"># 逐一比较Tensor1中的元素是否大于等于Tensor2中的元素</span></span><br><span class="line"><span class="built_in">print</span>(torch.ge(x,y))</span><br><span class="line"><span class="comment"># 逐一比较Tensor1中的元素是否大于Tensor2中的元素</span></span><br><span class="line"><span class="built_in">print</span>(torch.gt(x,y))</span><br><span class="line"><span class="comment"># 逐一比较Tensor1中的元素是否小于等于Tensor2中的元素</span></span><br><span class="line"><span class="built_in">print</span>(torch.le(x,y))</span><br><span class="line"><span class="comment"># 逐一比较Tensor1中的元素是否小于Tensor2中的元素</span></span><br><span class="line"><span class="built_in">print</span>(torch.lt(x,y))</span><br><span class="line"><span class="comment"># 逐一比较两个Tensor中的元素是否不相等</span></span><br><span class="line"><span class="built_in">print</span>(torch.ne(x,y))</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;最大值、最小值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.Tensor([[<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>],[<span class="number">4</span>,<span class="number">7</span>,<span class="number">9</span>]])</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">max</span>(x))</span><br><span class="line"><span class="comment"># 若指定了dim，则返回最大值与最大值所对应的索引</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">max</span>(x,dim=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 最小值</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">min</span>(x))</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">min</span>(x,dim=<span class="number">0</span>))</span><br></pre></td></tr></table></figure>




      
    </div>
</article>

    </li>
  
    <li>
      <article class='ListView'>
    <header class="title">
      
        <h1>
          <a href="/2022/07/18/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Hadoop/">学习笔记-Hadoop</a>
        </h1>
      
      <div class='ListMeta'>
  <time datetime="2022-07-18T08:26:44.000Z" itemprop="datePublished">
    2022-07-18
  </time>
  
  
</div>

    </header>
    <div>
      
        
      
    </div>
</article>

    </li>
  
    <li>
      <article class='ListView'>
    <header class="title">
      
        <h1>
          <a href="/2022/07/18/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Flink/">学习笔记-Flink</a>
        </h1>
      
      <div class='ListMeta'>
  <time datetime="2022-07-18T08:26:32.000Z" itemprop="datePublished">
    2022-07-18
  </time>
  
  
</div>

    </header>
    <div>
      
        <p><a target="_blank" rel="noopener" href="https://flink.apache.org/">Flink官网</a></p>
<p>学习Flink之前先了解两个概念：无界流和有界流<br>任何类型的数据都是作为事件流产生的。信用卡交易，传感器测量，机器日志或网站/移动应用程序上的用户交互，所有这些数据都作为流生成。<br>数据可以作为无界或有界流处理。<br><strong>无界流</strong>：有一个开始但是没有结束，它们不会在生成时终止并提供数据，必须连续处理无界流，也就是说必须在获取后立即处理event。对于无界数据流我们无法等待所有数据都到达，因为输入是无界的，并且在任何时间点都不会完成。处理无界数据通常要求以特定顺序（例如事件发生的顺序）获取event，以便能够推断结果完整性。<br><strong>有界流</strong>：有界数据流有明确定义的开始和结束，可以在执行任何计算之前通过获取所有数据来处理有界流，处理有界流不需要有序获取，因为可以始终对有界数据集进行排序，有界流的处理也称为批处理。</p>
<p><strong>Flink简介</strong><br>Apache Flink是一个框架和分布式处理引擎，用于对无界和有界数据进行有状态计算。Flink设计为在所有常见的集群环境中进行，以内存速度和任何规模执行计算。</p>

      
    </div>
</article>

    </li>
  
    <li>
      <article class='ListView'>
    <header class="title">
      
        <h1>
          <a href="/2022/07/18/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Spark/">学习笔记-Spark</a>
        </h1>
      
      <div class='ListMeta'>
  <time datetime="2022-07-18T08:26:20.000Z" itemprop="datePublished">
    2022-07-18
  </time>
  
  
</div>

    </header>
    <div>
      
        
      
    </div>
</article>

    </li>
  
    <li>
      <article class='ListView'>
    <header class="title">
      
        <h1>
          <a href="/2022/07/13/Linux-awk%E5%91%BD%E4%BB%A4/">Linux-awk命令</a>
        </h1>
      
      <div class='ListMeta'>
  <time datetime="2022-07-13T09:28:05.000Z" itemprop="datePublished">
    2022-07-13
  </time>
  
  | 
  <ul>
    
  <li class="meta-text">
  { <a href="/tags/Linux/">Linux</a> }
  </li>


  </ul>
  
  
</div>

    </header>
    <div>
      
        <p>awk是一个强大的文本分析工具，可以对文本进行分析并生成报告。</p>
<p>工作原理：逐行读取文本，默认以空格或tab键为分隔符进行分隔，将分隔所得的各个字段保存到内建变量中，并按模式或者田间执行编辑命令。</p>
<p>sed命令常用于一整行的处理，而awk比较倾向于将一行分成多个“字段”然后再进行处理。awk信息的读入也是逐行读取的，执行结果可以通过print的功能将字段数据打印显示。在使用awk命令的过程中,可以使用逻辑操作符“&amp;&amp;”表示“与”、“||”表示“或”、“!”表示“非”；还可以进行简单的数学运算，如+、-、*、/、%、^分别表示加、减、乘、除、取余和乘方。</p>
<p>awk常见的内建变量（可直接用）如下所示<br>FS：列分割符。指定每行文本的字段分隔符，默认为空格或制表位。与”-F”作用相同<br>NF：当前处理的行的字段个数。<br>NR：当前处理的行的行号（序数）。<br>$0：当前处理的行的整行内容。<br>$n：当前处理行的第n个字段（第n列）。<br>FILENAME：被处理的文件名。<br>RS：行分隔符。awk从文件上读取资料时,将根据RS的定义把资料切割成许多条记录,而awk一次仅读入一条记录,以进行处理。预设值是’\n’</p>
<p>使用示例：<br>awk ‘{print}’ 1.txt        #输出所有内容<br>awk ‘{print $0}’ 1.txt        #输出所有内容<br>awk ‘NR==1,NR==3{print}’ 1.txt        #输出第1至3 行内容<br>awk ‘(NR&gt;=1)&amp;&amp;(NR&lt;=3){print}’ 1.txt        #输出第 1至3 行内容<br>awk ‘NR==1||NR==3{print}’ testfile2        #输出第1行、第3行内容<br>awk ‘(NR%2)==1{print}’ testfile2        #输出所有奇数行的内容<br>awk ‘(NR%2)==0{print}’ testfile2        #输出所有偶数行的内容<br>awk ‘/^root/{print}’ /etc/passwd        #输出以 root 开头的行<br>awk ‘/nologin$/{print}’ /etc/passwd        #输出以 nologin 结尾的行<br>awk -F “:” ‘{print NR,$0}’ /etc/passwd        #输出每行内容和行号，每处理完一条记录，NR值加1<br>awk -F “:” ‘$7~“/bash”{print $1}’ /etc/passwd        #输出以冒号分隔且第7个字段中包含/bash的行的第1个字段<br>awk -F “:” ‘($1~“root”)&amp;&amp;(NF==7){print $1,$2}’ /etc/passwd        #输出第1个字段中包含root且有7个字段的行的第1、2个字段<br>awk -F ‘\t’ ‘FILENAME~/predict/{if($2!=”NULL”)cid[$2]=$1}FILENAME~/label/{if($1 in cid)print cid[$1], $0}’ OFS=’\t’ predict.txt label.txt</p>

      
    </div>
</article>

    </li>
  
    <li>
      <article class='ListView'>
    <header class="title">
      
        <h1>
          <a href="/2022/07/13/%E7%AE%97%E6%B3%95-TextCNN/">算法-TextCNN</a>
        </h1>
      
      <div class='ListMeta'>
  <time datetime="2022-07-13T06:14:14.000Z" itemprop="datePublished">
    2022-07-13
  </time>
  
  | 
  <ul>
    
  <li class="meta-text">
  { <a href="/tags/Algorithm/">Algorithm</a> }
  </li>


  </ul>
  
  
</div>

    </header>
    <div>
      
        <p>2012年在深度学习和卷积神经网络成为图像任务明星之后，2014年TextCNN诞生于世，成为了CNN在NLP文本分类任务上的经典之作。TextCNN提出的目的在于，希望将CNN在图像领域中所取得的成就复制于自然语言处理NLP任务中。</p>
<p>TextCNN利用卷积来提取文本n-gram特征，最大池化，全连接然后进行分类。</p>

      
    </div>
</article>

    </li>
  
    <li>
      <article class='ListView'>
    <header class="title">
      
        <h1>
          <a href="/2022/07/13/%E8%AE%BA%E6%96%87-MarkBERT-Marking-Word-Boundaries-Improves-Chinese-BERT/">论文-MarkBERT: Marking Word Boundaries Improves Chinese BERT</a>
        </h1>
      
      <div class='ListMeta'>
  <time datetime="2022-07-12T16:56:17.000Z" itemprop="datePublished">
    2022-07-13
  </time>
  
  | 
  <ul>
    
  <li class="meta-text">
  { <a href="/tags/pretrained-model/">pretrained model</a> }
  </li>


  </ul>
  
  
  / 
  <ul>
    
  <li class="meta-text">
  { <a href="/categories/paper-reading/">paper reading</a> }
  </li>


  </ul>
  
</div>

    </header>
    <div>
      
        <p>作者单位：腾讯AI实验室、复旦大学</p>
<p><strong>摘要</strong><br>&emsp;本文提出了一种基于词信息的中文BERT模型——MarkBERT模型。现有的基于单词的BERT模型以单词为基本单位，但由于BERT的词汇量限制，模型只涵盖高频单词，遇到词汇量不足(OOV)的单词时，模型退回到字符层面。与现有作品不同的是，MarkBERT将词汇表保持为汉字，并在相邻的单词之间插入边界标记。这样的设计使模型能够以相同的方式处理任何单词，无论它们是否是OOV单词。此外，我们的模型还有两个额外的好处:第一，它方便在标记的基础上增加词汇水平的学习目标，这是对传统的汉字和句子水平训练前任务的补充;其次，它可以通过将通用标记替换为特定于POS标记的标记，方便地合并更丰富的语义，如单词的POS标记。MarkBERT在MSRA数据集和OntoNotes数据集上的中文命名实体识别水平分别从95.4%和82.8%提高到96.5%和84.2%。与以往基于单词的BERT模型相比，MarkBERT模型在文本分类、关键字识别和语义相似度任务方面取得了更好的准确性。</p>

      
    </div>
</article>

    </li>
  
</ul>

  <section id="nav-wrapper">
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">next »</a>
    </nav>
  </section>


            <footer>
    <div>© 2022 - John Doe </div>
    <div>
        <span>
            Powered by <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a>
        </span>
        ,
        <span>
            Theme - <a target="_blank" rel="noopener" href="https://github.com/nameoverflow/hexo-theme-icalm">Icalm</a>
        </span>
    </div>
</footer>

        </div>
    </div>
</div>

<script src="/js/pager/dist/singlepager.js"></script>

<script>
var sp = new Pager('data-pager-shell')

</script>
</body>
</html>