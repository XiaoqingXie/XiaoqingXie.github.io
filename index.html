<!DOCTYPE html>
<html lang="zh-cmn-Hans">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Xiaoqing Xie</title>
  
  
  
  <!--link rel="stylesheet" href="//cdn.jsdelivr.net/highlight.js/9.10.0/styles/github-gist.min.css"-->
  
<link rel="stylesheet" href="//cdn.jsdelivr.net/highlight.js/9.10.0/styles/github-gist.min.css">

  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 5.4.2"></head>

<body>
<div class="Shell">
    <aside class='SideBar'>
    <section class='avatar' style="background-image: url()">
        <div class='av-pic' style="background-image: url(/assets/myphoto.png)">
        </div>
    </section>
    <section class='menu'>
        <div>Xiaoqing Xie</div>
        
        <ul>
          
            <a href="/Resume/" class="Btn">
              <li>Resume</li>
            </a>  
          
            <a href="/" class="Btn">
              <li>Blogs</li>
            </a>  
          
            <a href="/archives/" class="Btn">
              <li>Archive</li>
            </a>  
          
            <a href="/tags/" class="Btn">
              <li>Tags</li>
            </a>  
          
            <a href="/categories/" class="Btn">
              <li>Categories</li>
            </a>  
          
            <a href="/about/" class="Btn">
              <li>About</li>
            </a>  
          
        </ul>
    </section>
    <section class="media">
        
            
                <a target="_blank" rel="noopener" href="https://github.com/XiaoqingXie">
                    <img src="/assets/github.svg" />
                </a>
            
        
    </section>
</aside>

    <div class="container">
        <div data-pager-shell>
            <ul class="Index">
  
  
    <li>
      <article class='ListView'>
    <header class="title">
      
        <h1>
          <a href="/2022/09/15/%E5%B0%86tensorflow%E6%A8%A1%E5%9E%8B%E6%89%93%E5%8C%85%E6%88%90PB%E6%96%87%E4%BB%B6%E5%8F%8APB%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96%E6%96%B9%E5%BC%8F/">将tensorflow模型打包成PB文件及PB文件读取方式</a>
        </h1>
      
      <div class='ListMeta'>
  <time datetime="2022-09-15T11:44:00.000Z" itemprop="datePublished">
    2022-09-15
  </time>
  
  
</div>

    </header>
    <div>
      
        <p>完成模型训练之后，为了使模型可以在线上稳定运行，一般需要将其转换成pb文件。</p>
<ol>
<li>tensorflow模型文件打包成pb文件<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="string">&quot;&quot;</span>   <span class="comment"># 禁止使用GPU</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line">sys.path.insert(<span class="number">0</span>, <span class="built_in">str</span>(Path.cwd().parents[<span class="number">0</span>].parents[<span class="number">0</span>].parents[<span class="number">0</span>]) )</span><br><span class="line">sys.path.insert(<span class="number">0</span>, <span class="built_in">str</span>(Path.cwd().parents[<span class="number">0</span>].parents[<span class="number">0</span>]))     <span class="comment"># 适配直接cmd运行, 即直接 python model_train.py</span></span><br><span class="line">sys.path.insert(<span class="number">0</span>, <span class="built_in">str</span>(Path.cwd().parents[<span class="number">0</span>]) )</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> graph_util</span><br><span class="line"><span class="comment">#from Recognition64_code.model.recognize64_model import recognize64Model</span></span><br><span class="line"><span class="comment">#from Recognition64_code.components.model_components.model_config import modelConfig</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">tfModel2pdFile</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将tensorflow模型转换为pb文件</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将tf模型转换为pb文件</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.semantics_model_path = <span class="string">&quot;&quot;</span></span><br><span class="line">        self.pb_model_dir = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">model2pb</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># semantics模型转pb</span></span><br><span class="line">        checkpoint_path = os.path.join(self.semantics_model_path, <span class="string">&quot;best_validation&quot;</span>)</span><br><span class="line">        output_pb_path = os.path.join(self.pb_model_dir, <span class="string">&quot;model.pb&quot;</span>)</span><br><span class="line">        output_node_names = [<span class="string">&quot;input_x&quot;</span>, <span class="string">&quot;keep_prob&quot;</span>,<span class="string">&quot;score/predict&quot;</span>]</span><br><span class="line"></span><br><span class="line">        self._model2pb(checkpoint_path, output_pb_path, output_node_names)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_model2pb</span>(<span class="params">self, ckpt_path, output_pb_path, output_node_names</span>):</span><br><span class="line">        <span class="comment"># 载入拼音模型</span></span><br><span class="line">        <span class="comment"># self.get_all_node(checkpoint_path, &quot;./pinyin_nodes.txt&quot;)    # 当不知道output_node_names时, 可保存所有节点查找.</span></span><br><span class="line"></span><br><span class="line">        saver = tf.train.import_meta_graph(<span class="string">&quot;&#123;&#125;.meta&quot;</span>.<span class="built_in">format</span>(ckpt_path))</span><br><span class="line">        graph = tf.get_default_graph()   <span class="comment"># 获取默认的图</span></span><br><span class="line">                input_graph_def = graph.as_graph_def()   <span class="comment"># 返回一个序列化的图代表当前图</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">            sess.run(tf.global_variables_initializer())</span><br><span class="line">            saver.restore(sess, ckpt_path)       <span class="comment"># 将checkpoint载入到sess</span></span><br><span class="line">            output_graph_def = graph_util.convert_variables_to_constants(sess,input_graph_def=input_graph_def,</span><br><span class="line">                                                                     output_node_names = output_node_names)</span><br><span class="line">            <span class="keyword">with</span> tf.gfile.FastGFile(output_pb_path,<span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(output_graph_def.SerializeToString())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_all_node</span>(<span class="params">self, ckpt_path, save_path</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将ckpt_path中的所有节点打印保存到save_path中.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        saver = tf.train.import_meta_graph(ckpt_path + <span class="string">&#x27;.meta&#x27;</span>, clear_devices=<span class="literal">True</span>)</span><br><span class="line">        graph = tf.get_default_graph()</span><br><span class="line">        input_graph_def = graph.as_graph_def()</span><br><span class="line">        <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">            file=<span class="built_in">open</span>(save_path,<span class="string">&#x27;a+&#x27;</span>)</span><br><span class="line">            <span class="keyword">for</span> n <span class="keyword">in</span> tf.get_default_graph().as_graph_def().node:</span><br><span class="line">                file.write(n.name + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">            file.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;开始进行模型的转换&quot;</span>)</span><br><span class="line">    tf2pb = tfModel2pdFile()</span><br><span class="line">    tf2pb.model2pb()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;模型转换成功！&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ol>

      
    </div>
</article>

    </li>
  
    <li>
      <article class='ListView'>
    <header class="title">
      
        <h1>
          <a href="/2022/09/12/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E4%BF%A1%E6%81%AF%E7%AE%80%E5%8F%B2/">读书笔记-信息简史</a>
        </h1>
      
      <div class='ListMeta'>
  <time datetime="2022-09-11T16:30:31.000Z" itemprop="datePublished">
    2022-09-12
  </time>
  
  
</div>

    </header>
    <div>
      
        <p>引言<br>1948年，贝尔实验室发明了晶体管。晶体管引发了电子产业的革命，为电子技术的微型化和普遍应用开辟了道路。另一项发明是对信息定义了单位“比特”，来源于年仅32岁的香农写的论文《通信的数学理论》。<br>进化生物学家理查德道金斯认为“处于所有生物核心的不是火，不是热气，也不是所谓的‘生命火花’，而是信息、字词以及指令……如果你想了解生命，就别去研究那些生机勃勃、动来动去的原生质了，从信息技术的角度想想吧。”生物体中的所有细胞都是一个错综复杂的通信网络中的节点，它们一刻不停地传播和接受信息，不停地编码和解码。进化本身正是生物体和环境之间持续不断的信息交换的具体表现。<br>当光子、电子以及其他基本粒子发生相互作用时，它们实际是在做什么呢？其实是在交换比特、转换量子态以及处理信息，而物理定律就是处理信息时所用的算法。因此，每一颗正在燃烧的恒星、每一个星云、每一粒在云室中留下幽灵般痕迹的粒子，都是一台信息处理器，而宇宙也在计算着自己的命运。</p>
<p>第一章<br>会说话的鼓（似是而非的编码）<br>和非洲敲鼓传递信息相比，作者先介绍了烽火、磁石、摩斯密码等古老的传递信息的方法。</p>

      
    </div>
</article>

    </li>
  
    <li>
      <article class='ListView'>
    <header class="title">
      
        <h1>
          <a href="/2022/08/19/Linux-iconv%E5%91%BD%E4%BB%A4/">Linux-iconv命令</a>
        </h1>
      
      <div class='ListMeta'>
  <time datetime="2022-08-19T07:02:01.000Z" itemprop="datePublished">
    2022-08-19
  </time>
  
  
</div>

    </header>
    <div>
      
        
      
    </div>
</article>

    </li>
  
    <li>
      <article class='ListView'>
    <header class="title">
      
        <h1>
          <a href="/2022/08/18/%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0%E5%90%88%E9%9B%86/">刷题笔记合集</a>
        </h1>
      
      <div class='ListMeta'>
  <time datetime="2022-08-18T06:37:26.000Z" itemprop="datePublished">
    2022-08-18
  </time>
  
  
</div>

    </header>
    <div>
      
        
      
    </div>
</article>

    </li>
  
    <li>
      <article class='ListView'>
    <header class="title">
      
        <h1>
          <a href="/2022/08/18/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Graph%E7%AE%97%E6%B3%95%E5%9C%A8%E6%8A%96%E9%9F%B3%E9%A3%8E%E6%8E%A7%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/">学习笔记-Graph算法在抖音风控中的应用</a>
        </h1>
      
      <div class='ListMeta'>
  <time datetime="2022-08-18T06:27:21.000Z" itemprop="datePublished">
    2022-08-18
  </time>
  
  
</div>

    </header>
    <div>
      
        <p><strong>一、关联网络</strong><br>&emsp;1.关联网络-Object实体<br>&emsp;&emsp;异构图：用户节点和Object节点<br>&emsp;&emsp;同构图：只考虑用户节点<br>&emsp;2.关联网络-UGC相似<br>&emsp;&emsp;建立黑库，黑库与新发布内容做相似计算<br>&emsp;3.关联网络-行为相似<br>&emsp;&emsp;如何利用相似性构图？<br>&emsp;&emsp;如何描述两个行为序列的相似性？<br>&emsp;&emsp;&emsp;&emsp;对于2个行为序列，统计所有的K-gram，对于出现频率归一化，作为最后的特征Feature，将余弦距离作为最后的度量大小。（<strong>转换成向量计算</strong>）<br>&emsp;4.关联网络-风险实体<br>&emsp;&emsp;用户-联系方式-用户<br>&emsp;&emsp;用户-发布链接-用户<br>&emsp;5.关联网络-属性信息<br>&emsp;&emsp;UFA算法<br>&emsp;6.关联网络-社交关系<br>&emsp;7.关联网络-大规模相似计算<br>&emsp;&emsp;LSH: locality-sensitive hashing</p>
<p><strong>二、图计算</strong><br>&emsp;1.图计算-风险扩散<br>&emsp;&emsp;基于给定的图网络和黑用户种子，如何发现更多的黑用户？<br>&emsp;&emsp;如果种子既有黑用户又有白用户？<br>&emsp;2.图计算-社区发现<br>&emsp;&emsp;Louvain<br>&emsp;&emsp;Infomap<br>&emsp;3.图计算-局部社区发现</p>
<p><strong>三、图学习</strong><br>&emsp;1.图学习-Graph Embedding<br>&emsp;&emsp;通过无监督的方式，将高维的节点局部图结构压缩为低维节点向量，并保留原始图特征的一种图学习技术，便于跟下游经典机器学习方法结合。<br>&emsp;2.图学习-节点分类</p>

      
    </div>
</article>

    </li>
  
    <li>
      <article class='ListView'>
    <header class="title">
      
        <h1>
          <a href="/2022/08/17/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">学习笔记-图神经网络</a>
        </h1>
      
      <div class='ListMeta'>
  <time datetime="2022-08-17T07:32:56.000Z" itemprop="datePublished">
    2022-08-17
  </time>
  
  
</div>

    </header>
    <div>
      
        
      
    </div>
</article>

    </li>
  
    <li>
      <article class='ListView'>
    <header class="title">
      
        <h1>
          <a href="/2022/08/08/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%85%AC%E5%BC%80%E8%AF%BE/">学习笔记-清华大学大模型公开课</a>
        </h1>
      
      <div class='ListMeta'>
  <time datetime="2022-08-08T05:45:00.000Z" itemprop="datePublished">
    2022-08-08
  </time>
  
  
</div>

    </header>
    <div>
      
        <p>Course Plan</p>
<ol>
<li>大模型基础知识<br>Lesson1 NLP &amp; Big Model Basics（GPU server, Linux, Bash, Conda, …)<br>Lesson2 Neural Network Basics(Pytorch<br>Lesson3 Transformer and PLMs(Huggingface Transformers)</li>
<li>大模型的关键技术<br>Lesson4 Prompt Tuning &amp; Delta Tuning(OpenPrompt, OpenDelta)<br>Lesson5 Efficient Training &amp; Model Compression(OpenBMB suite)<br>Lesson6 Big-Model-based Text understanding and generation</li>
<li>大模型在交叉学科的应用<br>Lesson7 Big Models X Biomedical Science<br>Lesson8 Big Models X Legal Intelligence<br>Lesson9 Big Models X Brain and Cognitive Science</li>
</ol>
<p><strong>Lesson1</strong><br>&emsp;&emsp;<strong>自然语言处理任务</strong><br>&emsp;&emsp;&emsp;&emsp;Part of speech, Named entity recognition, Co-reference, Basic dependencies, Knowledge Graph, Machine Translation, Sentiment Analysis and Opinion Mining, Computational Social Science, …</p>
<p>&emsp;&emsp;<strong>词表示</strong><br>&emsp;&emsp;&emsp;&emsp;词表示的意义<br>&emsp;&emsp;&emsp;&emsp;1）计算词相似度<br>&emsp;&emsp;&emsp;&emsp;2）推导出词与词之间的关系</p>
<p>&emsp;&emsp;<strong>语言模型</strong><br>&emsp;&emsp;N-gram<br>&emsp;&emsp;Neural Language Model</p>
<p>&emsp;&emsp;<strong>大模型基础</strong><br>&emsp;&emsp;大模型的训练范式:迁移学习<br>&emsp;&emsp;词嵌入模型(CBOW和Skip-Gram)是简单的预训练语言模型。</p>
<p>&emsp;&emsp;<strong>服务器和GPU使用基础知识</strong><br>&emsp;&emsp;Prerequisites：SSH, Linux命令, Vim, Tmux, 虚拟环境配置&amp;conda&amp;pip, Vscode(可实现远程连接), Git, Bash<br>&emsp;&emsp;Tmux可以用来管理进程。</p>
<p><strong>Lesson2</strong><br>&emsp;&emsp;<strong>神经网络基础</strong><br>&emsp;&emsp;<strong>神经网络的组成</strong>：神经元，矩阵表示，多层神经网络，前馈计算<br>&emsp;&emsp;常见的激活函数：Sigmoid, Tanh, ReLU……<br>&emsp;&emsp;输出层：<br>&emsp;&emsp;&emsp;线性输出<br>&emsp;&emsp;&emsp;Sigmoid（压到0-1之间）：用于二分类<br>&emsp;&emsp;&emsp;Softmax：用于多分类</p>
<p>&emsp;&emsp;<strong>如何训练神经网络</strong><br>&emsp;&emsp;设计训练目标<br>&emsp;&emsp;&emsp;均方差（常用于回归问题）<br>&emsp;&emsp;&emsp;交叉熵（常用于分类问题）<br>&emsp;&emsp;梯度下降<br>&emsp;&emsp;&emsp;如何得到梯度？求偏导数：链式法则<br>&emsp;&emsp;反向传播：下游梯度=上游梯度*本地梯度</p>
<p>&emsp;&emsp;<strong>词向量：Word2Vec</strong><br>&emsp;&emsp;CBOW<br>&emsp;&emsp;Skip-gram</p>
<p>&emsp;&emsp;<strong>循环神经网络RNN</strong><br>&emsp;&emsp;</p>
<p>&emsp;&emsp;<strong>门控循环单元GRU</strong><br>&emsp;&emsp;</p>
<p>&emsp;&emsp;<strong>长短期记忆网络LSTM</strong><br>&emsp;&emsp;</p>
<p>&emsp;&emsp;<strong>双向RNN</strong><br>&emsp;&emsp;</p>
<p>&emsp;&emsp;<strong>卷积神经网络CNN</strong><br>&emsp;&emsp;</p>
<p><strong>Lesson3</strong><br>&emsp;&emsp;<strong>Transformer和预训练语言模型</strong><br>&emsp;&emsp;<strong>注意力机制</strong><br>&emsp;&emsp;<strong>注意力的变体</strong><br>&emsp;&emsp;<strong>注意力的特点</strong></p>

      
    </div>
</article>

    </li>
  
    <li>
      <article class='ListView'>
    <header class="title">
      
        <h1>
          <a href="/2022/07/21/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Java/">学习笔记-Java</a>
        </h1>
      
      <div class='ListMeta'>
  <time datetime="2022-07-21T11:31:54.000Z" itemprop="datePublished">
    2022-07-21
  </time>
  
  
</div>

    </header>
    <div>
      
        <p>一、程序开发步骤说明<br>Java程序开发三步骤：编写、编译、运行。</p>
<p>public class后面代表定义一个类的名称，必须和文件名保持一致，类是Java当中所有源代码的基本组织单位。<br>main是程序执行的起点。</p>
<p>二、Java程序中关键字的概念和特征<br>关键字的概念：有特殊含义的、被保留的、不能随意使用的字符。</p>
<p>关键字的特点：<br>&emsp;&emsp;1. 完全小写的字母；<br>&emsp;&emsp;2. 在增强版的记事本中，有特殊的颜色。</p>
<p>三、标识符<br>概念：是指在程序中，我们自己定义的内容。比如类的名字、方法的名字和变量的名字等等，都是标识符。<br>命名规则：（硬性要求）<br>&emsp;&emsp;1. 标识符可以包含英文字母（区分大小写）、0-9数字、美元符号、下划线。<br>&emsp;&emsp;2. 标识符不能以数字开头。<br>&emsp;&emsp;3. 标识符不能是关键字。<br>命名规范：（软性建议）<br>&emsp;&emsp;1. 类名规范：首字母大写，后面每个单词首字母大写（大驼峰式）。<br>&emsp;&emsp;2. 变量名规范：首字母小写，后面每个单词首字母大写（小驼峰式）。<br>&emsp;&emsp;3. 方法名规范：同变量名。</p>
<p>四、常量<br>概念：在Java程序运行期间固定不变的数据。<br>常量的分类：<br>&emsp;&emsp;1. 字符串常量：凡事用双引号引起来的部分，叫做字符串常量。例如”abc”, “Hello”, “123”<br>&emsp;&emsp;2. 整数常量：直接写上的数字，没有小数点。例如：100，200，0，-250<br>&emsp;&emsp;3. 浮点数常量：直接写上的数字，有小数点。例如：2.5、-3.14、0.0<br>&emsp;&emsp;4. 字符常量：凡是用单引号引起来的单个字符，就是字符常量。例如，’A’、’b’、’9’、’中’ (单引号之间有且仅有一个字符，没有不行。）<br>&emsp;&emsp;5. 布尔常量：true, false<br>&emsp;&emsp;6. 空常量：null。代表没有任何数据。（空常量不能直接打印）</p>
<p>五、数据类型<br>基本数据类型：<br>&emsp;&emsp;整数型：byte short int(默认) long<br>&emsp;&emsp;浮点型：float double(默认)<br>&emsp;&emsp;字符型：char<br>&emsp;&emsp;布尔型：boolean<br>引用数据类型：字符串、数组、类、接口、lambda<br>&emsp;&emsp;</p>
<p>注意事项：<br>&emsp;&emsp;1. 字符串不是基本类型，而是引用型。<br>&emsp;&emsp;2. 浮点数可能只是一个近似值，并非精确的值。<br>&emsp;&emsp;3. 数据范围与字节数不一定相关，例如float数据范围比long更加广泛，但是float是4字节，long是8字节。<br>&emsp;&emsp;4. 浮点数当中默认类型是double。如果一定要用float类型，需要加上一个后缀F。<br>&emsp;&emsp;   如果是整数，默认为int类型，如果一定要使用long类型，需要加上一个后缀L。推荐使用大写字母后缀。</p>
<p>六、变量<br>程序运行期间，可以发生改变的量。</p>
<p>数据类型 变量名称; //创建了一个变量</p>
<p>数据类型 变量名称 = 数据值; //在创建一个变量的同时，立刻放入制定的数值</p>
<p>注意事项：<br>&emsp;&emsp;1. 如果创建多个变量，变量之间的名称不能重复<br>&emsp;&emsp;2. 对于float和long类型来说，字母后面的F和L不要丢掉<br>&emsp;&emsp;3. 如果使用byte或者short类型的变量，右侧的数据值不能超过左侧类型的范围<br>&emsp;&emsp;4. 没有进行赋值的变量，不能直接使用；一定要赋值之后才能使用<br>&emsp;&emsp;5. 变量使用不能超过作用域的范围（作用域：从定义变量的一行开始，一直到直接所属的大括号结束为止）<br>&emsp;&emsp;6. 可以通过一个语句创建多个变量，但是一般情况不建议这么写。</p>
<p>七、数据类型转换<br>自动转换</p>
<p>强制转换</p>
<p>注意事项：<br>&emsp;&emsp;<br>&emsp;&emsp;<br>&emsp;&emsp;<br>&emsp;&emsp;</p>

      
    </div>
</article>

    </li>
  
    <li>
      <article class='ListView'>
    <header class="title">
      
        <h1>
          <a href="/2022/07/19/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Tensorflow/">学习笔记-Tensorflow</a>
        </h1>
      
      <div class='ListMeta'>
  <time datetime="2022-07-19T08:25:22.000Z" itemprop="datePublished">
    2022-07-19
  </time>
  
  
</div>

    </header>
    <div>
      
        
      
    </div>
</article>

    </li>
  
    <li>
      <article class='ListView'>
    <header class="title">
      
        <h1>
          <a href="/2022/07/19/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Pytorch/">学习笔记-Pytorch</a>
        </h1>
      
      <div class='ListMeta'>
  <time datetime="2022-07-19T08:25:10.000Z" itemprop="datePublished">
    2022-07-19
  </time>
  
  
</div>

    </header>
    <div>
      
        <p><strong>Pytorch vs Tensorflow</strong><br>Pytorch:简洁、动态计算、visdom、部署不方便<br>Tensorflow: 接口复杂、静态图(TF2.0 Eager Execution已经引入动态图)、Tensorboard、部署方便(TF serving)</p>
<p><strong>Pytorch安装与环境搭建</strong><br>Ubuntu16.04–CUDA+cuDNN–Python3+pip3/Anaconda–Pytorch<br>硬件要求：GTX1080Ti+16G内存</p>
<p><strong>Pytorch的基本概念</strong></p>
<ol>
<li><p>Tensor 张量<br>&emsp;使用Tensor对样本进行描述<br>&emsp;使用Tensor对模型中的变量进行描述<br>&emsp;Tensor的创建<br>&emsp;Tensor(*size)    基础构造函数<br>&emsp;Tensor(data)    类似np.array<br>&emsp;标量是零维的张量，向量是一维的张量，矩阵是二维的张量。<br>&emsp;<strong>Tensor可以用来描述机器学习中的样本或者模型。</strong></p>
</li>
<li><p>Tensor创建编程实例</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.Tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(a.<span class="built_in">type</span>())</span><br></pre></td></tr></table></figure>
<br>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a = torch.Tensor(<span class="number">2</span>,<span class="number">3</span>) <span class="comment"># 指定tensor的形状</span></span><br><span class="line">b = torch.ones(<span class="number">2</span>,<span class="number">2</span>) <span class="comment"># 指定形状，初始化为全1的tensor</span></span><br><span class="line">c = torch.eye(<span class="number">2</span>,<span class="number">2</span>) <span class="comment"># 指定形状，初始化为对角线的tensor</span></span><br><span class="line">d = torch.zeros(<span class="number">2</span>,<span class="number">2</span>) <span class="comment"># 指定形状，初始化为全0的tensor</span></span><br><span class="line">a1 = torch.zeros_like(a) <span class="comment"># 以tensor a的形状，初始化一个全0的tensor</span></span><br><span class="line">a2 = torch.ones_like(a)</span><br><span class="line">a3 = torch.eye_like(a)</span><br><span class="line"><span class="comment"># 随机</span></span><br><span class="line">e = torch.rand(<span class="number">2</span>,<span class="number">2</span>) <span class="comment"># 指定tensor的形状,生成随机值的tensor</span></span><br><span class="line">f = torch.normal(mean=<span class="number">0.0</span>, std=torch.rand(<span class="number">5</span>))</span><br><span class="line">g = torch.Tensor(<span class="number">2</span>,<span class="number">2</span>).uniform(-<span class="number">1</span>,<span class="number">1</span>) <span class="comment">#先指定形状，再定义从-1到1之间的均匀分布tensor</span></span><br><span class="line"><span class="comment"># 序列</span></span><br><span class="line">h = torch.arange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">1</span>) <span class="comment"># 从0-9的间隔1的tensor(long tensor)</span></span><br><span class="line">i = torch.linspace(<span class="number">2</span>, <span class="number">10</span>, <span class="number">3</span>) <span class="comment"># 等差tensor</span></span><br><span class="line">j = torch.randperm(<span class="number">10</span>) <span class="comment"># 从0-9打乱的tensor(long tensor)</span></span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;numpy定义数据的实例<br>&emsp;&emsp;用法与Tensor类似</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure>
<br>

<ol start="3">
<li>Tensor的属性</li>
</ol>
<p>&emsp;&emsp;每一个Tensor有torch.dtype、torch.device、torch.layout三种属性。<br>&emsp;&emsp;torch.device标识了torch.Tensor对象再创建后所存储在的设备名称(cpu/gpu)。<br>&emsp;&emsp;torch.layout表示torch.Tensor内存布局对象。<br>&emsp;&emsp;例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tnesor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],dtype=torch.float32,device=torch.device(<span class="string">&#x27;cpu&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;稀疏的张量的定义<br>&emsp;&emsp;torch.sparse_coo_tensor<br>&emsp;&emsp;coo类型表示了非零元素的坐标形式<br>&emsp;&emsp;例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">dev = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">indices = torch.tensor([<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]))  <span class="comment"># 坐标</span></span><br><span class="line">values = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])。<span class="comment"># 值</span></span><br><span class="line">x = torch.sparse_coo_tensor(indices, values, [<span class="number">4</span>,<span class="number">4</span>], dtype=torch.float32, device=dev) <span class="comment"># 对角线上有值的稀疏张量</span></span><br><span class="line">x = torch.sparse_coo_tensor(indices, values, [<span class="number">4</span>,<span class="number">4</span>], dtype=torch.float32, device=dev)。to_dense()  <span class="comment"># 转换成稠密张量</span></span><br></pre></td></tr></table></figure>
<br>

<ol start="4">
<li>Tensor的算术运算</li>
</ol>
<p>&emsp;&emsp;<strong>四则运算</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加法</span></span><br><span class="line">c = a + b</span><br><span class="line">c = torch.add(a, b)</span><br><span class="line">a.add(b)</span><br><span class="line">a.add_(b) <span class="comment"># 会对a的值修改</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 减法</span></span><br><span class="line">c = a - b</span><br><span class="line">c = torch.sub(a, b)</span><br><span class="line">a.sub(b)</span><br><span class="line">a.sub_(b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 乘法(哈达玛积, element wise, 对应元素相乘)</span></span><br><span class="line">c = a * b</span><br><span class="line">c = torch.mul(a, b)</span><br><span class="line">a.mul(b)</span><br><span class="line">a.mul_(b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 除法</span></span><br><span class="line">c = a / b</span><br><span class="line">c = torch.div(a, b)</span><br><span class="line">a.div(b)</span><br><span class="line">a.div_(b)</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;<strong>矩阵运算</strong><br>&emsp;&emsp;二维矩阵乘法运算操作包括torch.mm()、torch.matmul()、@</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">b = torch.ones(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">torch.mm(a, b)</span><br><span class="line">torch.matmul(a, b)</span><br><span class="line">a @ b</span><br><span class="line">a.matmul(b)</span><br><span class="line">a.mm(b)</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;对于高位的Tensor(dim&gt;2)，定义其矩阵乘法仅在最后的两个维度上，要求前面的维度必须保持一致，就像矩阵的索引一样并且运算操作只有torch.matmul().</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">b = torch.ones(<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a.matmul(b))</span><br><span class="line"><span class="built_in">print</span>(torch.matmul(a,b))</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;幂运算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">pow</span>(a, <span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(a.<span class="built_in">pow</span>(<span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(a**<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(a.pow_(<span class="number">2</span>)) <span class="comment"># 结果赋给a</span></span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;e的n次方</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.exp(a))</span><br><span class="line">b = a.exp_()</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;开方运算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(a.sqrt())</span><br><span class="line"><span class="built_in">print</span>(a.sqrt_())</span><br><span class="line"><span class="built_in">print</span>(a)</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;对数运算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.log2(a))</span><br><span class="line"><span class="built_in">print</span>(torch.log10(a))</span><br><span class="line"><span class="built_in">print</span>(torch.log(a))</span><br><span class="line"><span class="built_in">print</span>(torch.log_(a))</span><br></pre></td></tr></table></figure>
<br>

<ol start="5">
<li>Pytorch的广播机制</li>
</ol>
<p>&emsp;&emsp;<strong>广播机制：</strong>张量参数可以自动扩展为相同大小<br>&emsp;&emsp;广播机制需要满足两个条件：<br>&emsp;&emsp;&emsp;1)每个张量至少有一个维度；<br>&emsp;&emsp;&emsp;2)满足右对齐:对两个张量的维度从后往前（从右向左）处理，维度的大小必须<strong>要么相等，要么其中一个为1</strong>，或者其中一个张量后面不存在维度了；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.empty(<span class="number">5</span>,<span class="number">7</span>,<span class="number">3</span>)</span><br><span class="line">y = torch.empty(<span class="number">5</span>,<span class="number">7</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 形状相同，可以广播</span></span><br><span class="line"></span><br><span class="line">x=torch.empty((<span class="number">0</span>,))</span><br><span class="line">y=torch.empty(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 不能广播，因为两个张量都必须只有一个维度</span></span><br><span class="line"></span><br><span class="line">x=torch.empty(<span class="number">5</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">y=torch.empty(  <span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 可以广播</span></span><br><span class="line"><span class="comment"># 1st trailing dimension: both have size 1</span></span><br><span class="line"><span class="comment"># 2nd trailing dimension: y has size 1</span></span><br><span class="line"><span class="comment"># 3rd trailing dimension: x size == y size</span></span><br><span class="line"><span class="comment"># 4th trailing dimension: y dimension doesn&#x27;t exist</span></span><br><span class="line"></span><br><span class="line">x=torch.empty(<span class="number">3</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">y=torch.empty(  <span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># x和y不能广播，因为倒数第三个维度大小不同，且不为1</span></span><br></pre></td></tr></table></figure>
<br>

<ol start="6">
<li>Tensor的取整/取余运算</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"> </span><br><span class="line">a = torch.rand(<span class="number">2</span>, <span class="number">2</span>).mul_(<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 向下取整</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;floor :&#x27;</span>,torch.floor(a))</span><br><span class="line"><span class="comment"># 向上取整</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;ceil :&#x27;</span>,torch.ceil(a))</span><br><span class="line"><span class="comment"># 四舍五入</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;round :&#x27;</span>,torch.<span class="built_in">round</span>(a))</span><br><span class="line"><span class="comment"># 只取整数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;trunc :&#x27;</span>,torch.trunc(a))</span><br><span class="line"><span class="comment"># 只取小数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;frac :&#x27;</span>,torch.frac(a))</span><br><span class="line"><span class="comment"># 取余数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;% :&#x27;</span>,a%<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<br>

<ol start="7">
<li>Tensor的比较运算</li>
</ol>
<p>&emsp;&emsp;等于、大于等于、大于、小于等于、小于、不相等</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x = torch.Tensor([[<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>],[<span class="number">4</span>,<span class="number">7</span>,<span class="number">9</span>]])</span><br><span class="line">y = torch.Tensor([[<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">4</span>,<span class="number">8</span>,<span class="number">9</span>]])</span><br><span class="line">z = torch.Tensor([[<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>],[<span class="number">4</span>,<span class="number">7</span>,<span class="number">9</span>]])</span><br><span class="line"><span class="built_in">print</span>(torch.eq(x,y))</span><br><span class="line"><span class="comment"># 比较两个Tensor是否相等</span></span><br><span class="line"><span class="built_in">print</span>(torch.equal(x,z))</span><br><span class="line"><span class="built_in">print</span>(torch.equal(x,y))</span><br><span class="line"><span class="comment"># 逐一比较Tensor1中的元素是否大于等于Tensor2中的元素</span></span><br><span class="line"><span class="built_in">print</span>(torch.ge(x,y))</span><br><span class="line"><span class="comment"># 逐一比较Tensor1中的元素是否大于Tensor2中的元素</span></span><br><span class="line"><span class="built_in">print</span>(torch.gt(x,y))</span><br><span class="line"><span class="comment"># 逐一比较Tensor1中的元素是否小于等于Tensor2中的元素</span></span><br><span class="line"><span class="built_in">print</span>(torch.le(x,y))</span><br><span class="line"><span class="comment"># 逐一比较Tensor1中的元素是否小于Tensor2中的元素</span></span><br><span class="line"><span class="built_in">print</span>(torch.lt(x,y))</span><br><span class="line"><span class="comment"># 逐一比较两个Tensor中的元素是否不相等</span></span><br><span class="line"><span class="built_in">print</span>(torch.ne(x,y))</span><br></pre></td></tr></table></figure>

<p>&emsp;&emsp;最大值、最小值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.Tensor([[<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>],[<span class="number">4</span>,<span class="number">7</span>,<span class="number">9</span>]])</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">max</span>(x))</span><br><span class="line"><span class="comment"># 若指定了dim，则返回最大值与最大值所对应的索引</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">max</span>(x,dim=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 最小值</span></span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">min</span>(x))</span><br><span class="line"><span class="built_in">print</span>(torch.<span class="built_in">min</span>(x,dim=<span class="number">0</span>))</span><br></pre></td></tr></table></figure>




      
    </div>
</article>

    </li>
  
</ul>

  <section id="nav-wrapper">
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">next »</a>
    </nav>
  </section>


            <footer>
    <div>© 2022 - John Doe </div>
    <div>
        <span>
            Powered by <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a>
        </span>
        ,
        <span>
            Theme - <a target="_blank" rel="noopener" href="https://github.com/nameoverflow/hexo-theme-icalm">Icalm</a>
        </span>
    </div>
</footer>

        </div>
    </div>
</div>

<script src="/js/pager/dist/singlepager.js"></script>

<script>
var sp = new Pager('data-pager-shell')

</script>
</body>
</html>